<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yi Hua</title>
  
  <meta name="author" content="Yi Hua">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data-old:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yi Hua</name>
              </p>
              <p>I am a PhD student at <a href="http://images-oldci.ece.cmu.edu">Image Science Lab</a> at Carnegie Mellon University, advised by Aswin Sankaranarayanan.
                During my PhD, I build compact lensless imagers and design 3D reconstruction algorithms for them.
              </p>
              <p><!-- 
                I received my M.S. in Computer Vision from Carnegie Mellon University in 2016, and B.S. in Computer Science from Rice University in 2015. 
                I interned at Google Daydream in 2018 and Apple Special Project Group in 2016.
              </p> -->
              <p>
                In my spare time, I paint watercolors, animate illustrations, and think about creative uses of computational photography.
              </p>
              <p style="text-align:center">
                <a href="mailto:huayi@cmu.com">Email</a> &nbsp/&nbsp
                <a href="data/YiHua_CV_040122.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=uB5001AAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/hawaiii/">Github</a> &nbsp/&nbsp
                <a href="https://hawaiiipaintspit.tumblr.com/">Tumblr</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/yi.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/yi.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in computational photography and computer vision.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images-old/clean_promo.jpg" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="http://imagesci.ece.cmu.edu/files/paper/2020/SweepCam_PAMI20.pdf">
                <papertitle>SweepCam ‚Äî Depth-aware Lensless Imaging using Programmable Masks</papertitle>
              </a>
              <br>
              <strong>Yi Hua</strong>, Shigeki Nakamura, <a href="https://intra.ece.ucr.edu/~sasif/">M. Salman Asif</a>,  <a href="http://imagesci.ece.cmu.edu">Aswin C. Sankaranarayanan</a> 
              <br>
              <em>TPAMI/ICCP, 2020</em>
              <br>
              <a href="https://www.youtube.com/watch?v=dcz1KwtSFoU">video</a> /
              <a href="http://imagesci.ece.cmu.edu/files/paper/2020/SweepCam_PAMI20_Supp.pdf">supp</a>

              <p>Recovering images from lensless measurements are challenging because the per-pixel depth of the scene needs to be correctly identified to avoid artifacts in the reconstruction. We introduce computational focusing on lensless measurements so that we can reconstruct one depth slice of the scene at a time, and improve the speed as well as the quality of reconstruction. </p>
            </td>
          </tr> 




          <!-- <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerfsuper_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images-old/nerf_supervision.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images-old/nerf_supervision.jpg' width="160">
              </div>
              <script type="text/javascript">
                function nerfsuper_start() {
                  document.getElementById('nerfsuper_image').style.opacity = "1";
                }

                function nerfsuper_stop() {
                  document.getElementById('nerfsuper_image').style.opacity = "0";
                }
                nerfsuper_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://waymo.com/research/block-nerf/">
                <papertitle>NeRF-Supervision: Learning Dense Object Descriptors from Neural Radiance Fields</papertitle>
              </a>
              <br>
              <a href="https://yenchenlin.me/">Lin Yen-Chen</a>, 
              <a href="http://www.peteflorence.com/">Pete Florence</a>, 
              <strong>Jonathan T. Barron</strong>,  <br>
              <a href="https://scholar.google.com/citations?user=_BPdgV0AAAAJ&hl=en">Tsung-Yi Lin</a>, 
              <a href="https://meche.mit.edu/people/faculty/ALBERTOR@MIT.EDU">Alberto Rodriguez</a>,
              <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>
              <br>
              <em>ICRA</em>, 2022  
              <br>
							<a href="http://yenchenlin.me/nerf-supervision/">project page</a> / 
							<a href="https://arxiv.org/abs/2203.01913">arXiv</a> / 
							<a href="https://www.youtube.com/watch?v=_zN-wVwPH1s">video</a> /
							<a href="https://github.com/yenchenlin/nerf-supervision-public">code</a> / 
							<a href="https://colab.research.google.com/drive/13ISri5KD2XeEtsFs25hmZtKhxoDywB5y?usp=sharing">colab</a>				
              <p></p>
              <p>NeRF works better than RGB-D cameras or multi-view stereo when learning object descriptors.</p>
            </td>
          </tr> -->
					
         <!--  <tr onmouseout="mip360_stop()" onmouseover="mip360_start()"  bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mip360_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images-old/mip360_sat.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images-old/mip360_sat.jpg' width="160">
              </div>
              <script type="text/javascript">
                function mip360_start() {
                  document.getElementById('mip360_image').style.opacity = "1";
                }

                function mip360_stop() {
                  document.getElementById('mip360_image').style.opacity = "0";
                }
                mip360_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://jonbarron.info/mipnerf360">
                <papertitle>Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields</papertitle>
              </a>
              <br>
              <strong>Jonathan T. Barron</strong>,
              <a href="https://bmild.github.io/">Ben Mildenhall</a>,
              <a href="https://scholar.harvard.edu/dorverbin/home">Dor Verbin</a>,
              <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
              <a href="https://phogzone.com/">Peter Hedman</a>
              <br>
							<em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="http://jonbarron.info/mipnerf360">project page</a>
              /
              <a href="https://arxiv.org/abs/2111.12077">arXiv</a>
              /
              <a href="https://youtu.be/zBSH-k9GbV4">video</a>
              <p></p>
              <p>mip-NeRF can be extended to produce realistic results on unbounded scenes.</p>
            </td>
          </tr>  -->

    <!-- <tr onmouseout="refnerf_stop()" onmouseover="refnerf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='refnerf_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images-old/refnerf.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images-old/refnerf.jpg' width="160">
        </div>
        <script type="text/javascript">
          function refnerf_start() {
            document.getElementById('refnerf_image').style.opacity = "1";
          }

          function refnerf_stop() {
            document.getElementById('refnerf_image').style.opacity = "0";
          }
          refnerf_stop()
        </script>
      </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://dorverbin.github.io/refnerf/index.html">
            <papertitle>Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields</papertitle>
          </a>
          <br>
          <a href="https://scholar.harvard.edu/dorverbin/home">Dor Verbin</a>,
          <a href="https://phogzone.com/">Peter Hedman</a>,
          <a href="https://bmild.github.io/">Ben Mildenhall</a>, <br>
          <a href="Todd Zickler">Todd Zickler</a>,
          <strong>Jonathan T. Barron</strong>,
          <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>
          <br>
    <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
          <br>
          <a href="https://dorverbin.github.io/refnerf/index.html">project page</a>
    /
          <a href="https://arxiv.org/abs/2112.03907">arXiv</a>
    /
          <a href="https://youtu.be/qrdRH9irAlk">video</a>
          <p></p>
          <p>Explicitly modeling reflections in NeRF produces realistic shiny surfaces and accurate surface normals, and lets you edit materials.</p>
        </td>
      </tr> -->

         <!--  <tr onmouseout="rawnerf_stop()" onmouseover="rawnerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='rawnerf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images-old/rawnerf.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images-old/rawnerf.jpg' width="160">
              </div>
              <script type="text/javascript">
                function rawnerf_start() {
                  document.getElementById('rawnerf_image').style.opacity = "1";
                }

                function rawnerf_stop() {
                  document.getElementById('rawnerf_image').style.opacity = "0";
                }
                rawnerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://bmild.github.io/rawnerf/index.html">
                <papertitle>NeRF in the Dark: High Dynamic Range View Synthesis from Noisy Raw images-old</papertitle>
              </a>
              <br>
              <a href="https://bmild.github.io/">Ben Mildenhall</a>,
              <a href="https://phogzone.com/">Peter Hedman</a>,
              <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>, <br>
              <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
              <strong>Jonathan T. Barron</strong>
              <br>
							<em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://bmild.github.io/rawnerf/index.html">project page</a>
        /
              <a href="https://arxiv.org/abs/2111.13679">arXiv</a>
        /
              <a href="https://www.youtube.com/watch?v=JtBS4KBcKVc">video</a>
              <p></p>
              <p>
								Properly training NeRF on raw camera data-old enables HDR view synthesis and bokeh, and outperforms multi-image denoising.</p>
            </td>
          </tr>  -->
					
   
         <!--  <tr onmouseout="regnerf_stop()" onmouseover="regnerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='regnerf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images-old/regnerf_after.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images-old/regnerf_before.jpeg' width="160">
              </div>
              <script type="text/javascript">
                function regnerf_start() {
                  document.getElementById('regnerf_image').style.opacity = "1";
                }

                function regnerf_stop() {
                  document.getElementById('regnerf_image').style.opacity = "0";
                }
                regnerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://m-niemeyer.github.io/regnerf/index.html">
                <papertitle>RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs</papertitle>
              </a>
              <br>
              <a href="https://m-niemeyer.github.io/">Michael Niemeyer</a>,
              <strong>Jonathan T. Barron</strong>,
              <a href="https://bmild.github.io/">Ben Mildenhall</a>, <br>
              <a href="https://msmsajjadi.github.io/">Mehdi S. M. Sajjadi</a>, 
              <a href="http://www.cvlibs.net/">Andreas Geiger</a>,
              <a href="http://www2.informatik.uni-freiburg.de/~radwann/">Noha Radwan</a>
              <br>
        <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://m-niemeyer.github.io/regnerf/index.html">project page</a>
        /
              <a href="https://arxiv.org/abs/2112.00724">arXiv</a>
        /
              <a href="https://www.youtube.com/watch?v=QyyyvA4-Kwc">video</a>
              <p></p>
              <p>Regularizing unseen views during optimization enables view synthesis from as few as 3 input images-old.</p>
            </td>
          </tr>  -->
          

        </tbody></table>

				
        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Misc</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images-old/cvf.jpg"></td>
            <td width="75%" valign="center">
              <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
              <br>
              <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Longuet-Higgins Award Committee Member, CVPR 2021</a>
              <br>
              <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
              <br>
              <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images-old/cs188.jpg" alt="cs188">
            </td>
            <td width="75%" valign="center">
              <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
              <br>
              <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
              <br>
              <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
            </td>
          </tr>
					

          <tr>
            <td align="center" style="padding:20px;width:25%;vertical-align:middle">
							<heading>Basically <br> Blog Posts</heading>
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
              <br>
              <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
              <br>
              <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
            </td>
          </tr> -->
					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                This website was adapted from <a href="https://github.com/jonbarron/jonbarron_website">source code</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
